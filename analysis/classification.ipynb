{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Cues on Internal States from the Movements of Natural Social Interactions\n",
    "\n",
    "**Dataset analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "#%matplotlib notebook\n",
    "#%matplotlib qt5\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0) # bigger figures\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set() # better looking figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our own set of small helper functions for plotting, etc\n",
    "from utils import plot_embedding, plot_compare_embeddings, show_heatmap, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset, rename and re-order columns where necessary. *Note that the participants who failed the attention check were excluded, and are not present in the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../fulldata.csv\")\n",
    "\n",
    "# re-order columns + keep only useful ones\n",
    "data = data[['pptID', 'condition', 'age', 'gender', 'nationality', 'firstLang', 'trial', 'clipId', 'freetext',\n",
    " 'q01', 'q02', 'q03', 'q04', 'q05', 'q06', 'q07', 'q08', 'q09', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16', 'q17',\n",
    " 'q18', 'q19', 'q20', 'q21', 'q22', 'q23', 'q24', 'q25', 'q26', 'q27', 'q28', 'q29', 'q30']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename `qXX` columns with the names of the actual constructs tested in the questionnaire.\n",
    "\n",
    "Notes:\n",
    "- `condition=1` is the 'Movement-only' (ie, skeletons) condition, `condition=2` is the 'Full-scene' condition\n",
    "- each participant `pptID` watched 4 different clips, hence 4 rows per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "constructs=[\"Sad\", \"Happy\", \"Angry\", \"Excited\", \"Calm\", \"Friendly\", \"Aggressive\", \"Engaged\", \"Distracted\", \"Bored\", \"Frustrated\",\"Dominant\",\"Submissive\"]\n",
    "\n",
    "index = data.columns.tolist()\n",
    "index = index[0:9] + [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + [c for c1 in constructs for c in ['left' + c1, 'right' + c1]]\n",
    "data.columns=index\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "For each left/right pair of constructs, compute the absolute difference and the sum (shifted to [-2, +2] interval).\n",
    "\n",
    "This provides insight on the imbalance of the given construct between the children (difference), and the overall 'strenght' of the construct in the clip (sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in constructs:\n",
    "    data[\"diff\"+c] = abs(data[\"left\" + c] - data[\"right\" + c])\n",
    "    data[\"sum\"+c] = data[\"left\" + c] + data[\"right\" + c] - 4\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 lists of columns names, one for diff/sum constructs (the main one), one for left/right constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsLeftRight=[]\n",
    "columnsDiffSum=[]\n",
    "\n",
    "for c in constructs:\n",
    "    columnsLeftRight.append(\"left\" + c)\n",
    "    columnsLeftRight.append(\"right\" + c)\n",
    "    \n",
    "    columnsDiffSum.append(\"diff\" + c)\n",
    "    columnsDiffSum.append(\"sum\" + c)\n",
    "\n",
    "\n",
    "# by default, work with differences & sum for each constructs\n",
    "selectedColumns=columnsDiffSum\n",
    "\n",
    "# work with differences & sum and the four questions about group dynamics\n",
    "allQuestionsDiffSum = [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + columnsDiffSum\n",
    "\n",
    "# work with left/right ratings and the four questions about group dynamics i.e. raw ratings\n",
    "allQuestionsLeftRight = [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + columnsLeftRight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define several useful 'partial' views of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL-SCENE DATA\n",
    "\n",
    "fullscene_df=data[data[\"condition\"]==2] # dataframe showing full scene data only\n",
    "\n",
    "# the responses to the 26 left/right Likert-scale questions\n",
    "fullscene_ratings_df=fullscene_df[selectedColumns].astype(float)\n",
    "fullscene=fullscene_ratings_df.values # the underlying numpy array, needed for clustering\n",
    "\n",
    "# clip names\n",
    "fullscene_labels=fullscene_df[\"clipId\"].values\n",
    "\n",
    "# mean ratings per clip\n",
    "fullscene_means=fullscene_df.groupby([\"clipId\"])[selectedColumns].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOVEMENT-ALONE DATA\n",
    "\n",
    "move_df=data[data[\"condition\"]==1] # dataframe showing movement alone data only\n",
    "\n",
    "# the responses to the 26 left/right Likert-scale questions\n",
    "move_ratings_df=move_df[selectedColumns].astype(float)\n",
    "move=move_ratings_df.values # the underlying numpy array, needed for clustering\n",
    "\n",
    "# clip names\n",
    "move_labels=move_df[\"clipId\"].values\n",
    "\n",
    "# mean ratings per clip\n",
    "move_means=move_df.groupby([\"clipId\"])[selectedColumns].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.1: Inter-Rater Agreement\n",
    "\n",
    "Calculate Kirppendorff's alpha to look at how much participants in each condition agreed on their ratings for each clip (lighter color means higher agreement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "\n",
    "krip={}\n",
    "\n",
    "for clipName, group in fullscene_df[[\"clipId\"] + allQuestionsLeftRight].groupby([\"clipId\"]): # working with all raw ratings\n",
    "    krip[int(clipName[0])]=(krippendorff.alpha(group.values[:,1:].astype(int),level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "for clipName, group in move_df[[\"clipId\"] + allQuestionsLeftRight].groupby(\"clipId\"):\n",
    "    krip[clipName]=krip[clipName] + (krippendorff.alpha(group.values[:,1:].astype(int),level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "for clipName, group in move_df[[\"clipId\"] + allQuestionsLeftRight].groupby(\"clipId\"):\n",
    "    ratings = group.values[:,1:].astype(int)\n",
    "    ratings = np.random.randint(0,5,ratings.shape)\n",
    "    krip[clipName]=krip[clipName] + (krippendorff.alpha(ratings,level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "    \n",
    "krippendorff_df=pd.DataFrame.from_dict(krip,orient=\"index\", columns=[\"Fullscene alpha\", \"N\", \"Movement-Alone alpha\", \"N\",\"Random ratings alpha\", \"N\"])\n",
    "\n",
    "show_heatmap(krippendorff_df[[\"Fullscene alpha\", \"Movement-Alone alpha\", \"Random ratings alpha\"]].round(3), cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the agreement scores in the fullscene vs movement-alone videos, using an Independent Samples T-test. \n",
    "\n",
    "Question: did participants in the fullscene condition agree more in their ratings of each clip than participants in the movement-alone condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from math import sqrt\n",
    "\n",
    "fullscene_krip = krippendorff_df[\"Fullscene alpha\"]\n",
    "move_krip = krippendorff_df[\"Movement-Alone alpha\"]\n",
    "\n",
    "print('Mean Kripp Alpha Fullscene:', fullscene_krip.mean())\n",
    "print('Mean Kripp Alpha Movement:', move_krip.mean())\n",
    "\n",
    "print('Paired Samples T-Test:', ttest_rel(fullscene_krip, move_krip))\n",
    "\n",
    "cohens_d = (fullscene_krip.mean() - move_krip.mean()) / (sqrt((fullscene_krip.std() ** 2 + move_krip.std() ** 2) / 2))\n",
    "\n",
    "print(\"Cohen's d:\", cohens_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chance level for Krippendorff's alpha, computed by replacing ratings for each clips and each conditions by random values, and averaging the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krip_max = 0\n",
    "\n",
    "for i in range(100):\n",
    "    krip_chance={}\n",
    "\n",
    "    for clipName, group in fullscene_df[[\"clipId\"] + allQuestionsLeftRight].groupby([\"clipId\"]): # working with all raw ratings\n",
    "        ratings = group.values[:,1:].astype(int)\n",
    "        ratings = np.random.randint(0,5,ratings.shape)\n",
    "        krip_chance[int(clipName[0])]=(krippendorff.alpha(ratings,level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "    for clipName, group in move_df[[\"clipId\"] + allQuestionsLeftRight].groupby(\"clipId\"):\n",
    "        ratings = group.values[:,1:].astype(int)\n",
    "        ratings = np.random.randint(0,5,ratings.shape)\n",
    "        krip_chance[clipName]=krip_chance[clipName] + (krippendorff.alpha(ratings,level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "\n",
    "    krippendorff_df=pd.DataFrame.from_dict(krip_chance,orient=\"index\", columns=[\"Fullscene alpha\", \"N\", \"Movement-Alone alpha\", \"N\"])\n",
    "    k = krippendorff_df[[\"Fullscene alpha\", \"Movement-Alone alpha\"]].values.mean()\n",
    "    if k > krip_max:\n",
    "        krip_max = k\n",
    "krip_max\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Automatic Labelling of Social Situations\n",
    "\n",
    "Multi-label classification using k-nearest neughbour (k=3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Values based on results from Classifier are subject to changes each time Classifier is run. As such, running the following analyses will not produce identical results to those reported in *Paper* but will produce similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "training_ground_truth = { '1': ['Aggressive'],\n",
    "                         '2': ['Excited', 'Aggressive', 'Aimless'],\n",
    "                         '3': ['Excited', 'Fun'],\n",
    "                         '4': ['Cooperative'],\n",
    "                         '5': ['Bored', 'Aimless'],\n",
    "                         '6': ['Cooperative'],\n",
    "                         '7': ['Dominant'],\n",
    "                         '8': ['Bored', 'Fun'],\n",
    "                         '9': ['Cooperative'],\n",
    "                         '10': ['Cooperative', 'Dominant'],\n",
    "                         '11': ['Cooperative', 'Dominant'],\n",
    "                         '12': ['Aggressive', 'Aimless'],\n",
    "                         '13': ['Excited', 'Aggressive', 'Aimless'],\n",
    "                         '14': ['Aggressive'],\n",
    "                         '15': ['Dominant'],\n",
    "                         '16': ['Cooperative', 'Dominant'],\n",
    "                         '17': ['Excited', 'Aggressive'],\n",
    "                         '18': ['Aggressive', 'Dominant'],\n",
    "                         '19': ['Dominant'],\n",
    "                         '20': ['Excited']}\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(training_ground_truth.values())\n",
    "\n",
    "def datasets(training=data, testing=None, cols=allQuestionsDiffSum, test_size=0.2, use_clip_id_as_label=False, random_labels=False):\n",
    "    \"\"\"Returns a training dataset and training labels, and a testing dataset and testing labels.\n",
    "    \n",
    "    If testing is None, it randomly splits the training dataframe (at test_size).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if testing is None:\n",
    "        \n",
    "        if use_clip_id_as_label:\n",
    "            labels = list(training[\"clipId\"].map(int))\n",
    "        else:\n",
    "            labels = []\n",
    "            for id in training[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[str(id)])\n",
    "\n",
    "        data = training[cols].values\n",
    "\n",
    "        training_data, testing_data, training_labels, testing_labels = train_test_split(data, labels, test_size=test_size, random_state=int(time.time()))\n",
    "\n",
    "        if not use_clip_id_as_label:\n",
    "            \n",
    "            training_labels, testing_labels = mlb.transform(training_labels), mlb.transform(testing_labels)\n",
    "            \n",
    "            if random_labels:\n",
    "                for labels in training_labels:\n",
    "                    np.random.shuffle(labels)                 \n",
    "                np.random.shuffle(training_labels)             \n",
    "            \n",
    "\n",
    "        return training_data, testing_data, training_labels, testing_labels\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if use_clip_id_as_label:\n",
    "            training_labels = list(training[\"clipId\"].map(int))\n",
    "            testing_labels = list(testing[\"clipId\"].map(int))\n",
    "        else:\n",
    "            labels = []\n",
    "            for id in training[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[str(id)])\n",
    "\n",
    "            training_labels = mlb.transform(labels)\n",
    "\n",
    "            labels = []\n",
    "            for id in testing[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[str(id)])\n",
    "\n",
    "            testing_labels = mlb.transform(labels)\n",
    "\n",
    "            if random_labels:\n",
    "                if random_labels:\n",
    "                    for labels in training_labels:\n",
    "                        np.random.shuffle(labels)                 \n",
    "                    np.random.shuffle(training_labels) \n",
    "\n",
    "        \n",
    "        training_data = training[cols].values\n",
    "        testing_data = testing[cols].values\n",
    "\n",
    "        return training_data, testing_data, training_labels, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def train(training_data, training_labels, layers_nb=3, layer_size=20):\n",
    "    \n",
    "    #clf = RandomForestClassifier()\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    #clf = ExtraTreeClassifier(random_state=0)\n",
    "\n",
    "    layers = (layer_size, ) * layers_nb\n",
    "    #print(\"Training a MLP classifier, layers: %s...\" % str(layers))\n",
    "\n",
    "    # => naively using a OneVsRestClassifier does not improve the classification results, on the contrary\n",
    "    #clf = OneVsRestClassifier(MLPClassifier(hidden_layer_sizes=layers, activation='relu', max_iter=1000, solver=\"lbfgs\"))\n",
    "    \n",
    "    #clf = MLPClassifier(hidden_layer_sizes=layers, activation='relu', max_iter=1000, solver=\"lbfgs\")\n",
    "    #clf = OneVsRestClassifier(SVC(kernel='rbf'))\n",
    "    \n",
    "    clf.fit(training_data, training_labels)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def predict(clf, testing_data, inverse_transform_labels=True):\n",
    "    p = clf.predict(testing_data)\n",
    "    if inverse_transform_labels:\n",
    "        return mlb.inverse_transform(p) \n",
    "    else:\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def run_classification(training, \n",
    "                       testing=None, \n",
    "                       cols=allQuestionsDiffSum, \n",
    "                       use_clip_id_as_label=False, \n",
    "                       random_labels=False,\n",
    "                       layers_nb=3,\n",
    "                       layer_size=20,\n",
    "                       iterations=50):\n",
    "    \"\"\"\n",
    "    Metrics for multi-label classification coming form Sorower, Mohammad S. \"A literature survey on algorithms for multi-label learning.\" Oregon State University, Corvallis (2010).\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\"Accuracy\": [],\n",
    "               #\"Jaccard similarity\" : [], # intersection over union\n",
    "              \"Precision\": [],\n",
    "               \"Recall\": [],\n",
    "              \"F1-measure\": []}              \n",
    "              #\"hamming_loss\": []} # not super useful, because as we have 'only' 7 labels, the hamming distance is never huge (max 7, and usually smaller), which make it a not-very-sensitive measure\n",
    "    labels_f1 = []\n",
    "    \n",
    "\n",
    "    for x in range(iterations):\n",
    "               \n",
    "        training_data, testing_data, training_labels, testing_labels = datasets(training=training, testing=testing, cols=cols, use_clip_id_as_label=use_clip_id_as_label, random_labels=random_labels)\n",
    "        \n",
    "        if x == 0:\n",
    "            print(\"Shape of training data: %s\" % str(training_data.shape))\n",
    "            print(\"Shape of testing data: %s\" % str(testing_data.shape))\n",
    "\n",
    "        #print(\"Fold %d/%d\" % (x+1, iterations))\n",
    "        \n",
    "        clf = train(training_data, training_labels, layers_nb=layers_nb, layer_size=layer_size)\n",
    "\n",
    "        #mean_score_exact += clf.score(testing_data, testing_labels)\n",
    "\n",
    "        pred_labels = predict(clf, testing_data, inverse_transform_labels = not use_clip_id_as_label)\n",
    "\n",
    "\n",
    "        \n",
    "        at_least_one = 0\n",
    "        at_least_one_no_incorrect = 0\n",
    "        \n",
    "        if not use_clip_id_as_label:\n",
    "            \n",
    "            nb_classes = len(mlb.classes_)\n",
    "            \n",
    "            labels_f1.append(dict(zip(mlb.classes_, metrics.f1_score(testing_labels, mlb.transform(pred_labels), average=None))))\n",
    "            \n",
    "            results[\"Accuracy\"].append(metrics.accuracy_score(testing_labels, mlb.transform(pred_labels)))\n",
    "            #results[\"Jaccard similarity\"].append(metrics.jaccard_similarity_score(testing_labels, mlb.transform(pred_labels)))\n",
    "            results[\"Precision\"].append(metrics.precision_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            results[\"Recall\"].append(metrics.recall_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            results[\"F1-measure\"].append(metrics.f1_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            #results[\"hamming_loss\"].append(metrics.hamming_loss(testing_labels, mlb.transform(pred_labels)))    \n",
    "            \n",
    "            \n",
    "            testing_labels = mlb.inverse_transform(testing_labels)\n",
    "            \n",
    "            exact = 0\n",
    "            accuracy = 0\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "            f1_measure = 0\n",
    "            \n",
    "            for actual, pred in zip(testing_labels, pred_labels):\n",
    "                \n",
    "                pred = set(pred)\n",
    "                actual = set(actual)\n",
    "                \n",
    "                if len(pred) == 0: continue\n",
    "                    \n",
    "                if pred == actual:\n",
    "                    #print(\"%s <-> %s\" % (actual, pred))\n",
    "                    exact += 1\n",
    "                    \n",
    "                intersection = pred.intersection(actual)\n",
    "                union = pred.union(actual)\n",
    "\n",
    "                #accuracy += float(len(intersection)) / len(union)\n",
    "                #precision += float(len(intersection)) / len(pred)\n",
    "                #recall += float(len(intersection)) / len(actual)\n",
    "                #f1_measure += 2 * float(len(intersection)) / (len(pred) + len(actual))\n",
    "                \n",
    "            \n",
    "            #results[\"exact\"].append(float(exact) / len(testing_labels))\n",
    "            #results[\"accuracy\"].append(accuracy / len(testing_labels))\n",
    "            #results[\"precision\"].append(precision / len(testing_labels))\n",
    "            #results[\"recall\"].append(recall / len(testing_labels))\n",
    "            #results[\"f1_measure\"].append(f1_measure / len(testing_labels))\n",
    "            \n",
    "            \n",
    "            \n",
    "        else: # use_clip_id_as_label = True\n",
    "            # does not make much sense as at_least_one & at_least_one_no_incorrect are the same as 'exact'\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(results), pd.DataFrame(labels_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP hyperparameters optimisation using a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for nb in range(1,6):\n",
    "    for size in range(2,21,2): \n",
    "        results_fullscene,labels_precision_fullscene = run_classification(fullscene_df, iterations=30,layers_nb=nb, layer_size=size)\n",
    "        res[\"(%d x %d)\" % (nb, size)] = results_fullscene\n",
    "\n",
    "grid_search=pd.concat(res, axis=1)\n",
    "grid_search.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_over_grid = grid_search.swaplevel(axis=1)[\"F1-measure\"].describe().transpose()\n",
    "accuracy_over_grid[\"mean\"].plot.bar(yerr=accuracy_over_grid[\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fullscene,labels_precision_fullscene = run_classification(fullscene_df, iterations=10,layers_nb=1, layer_size=6)\n",
    "results_fullscene.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene classification - 80%/20% split - multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels_f1_fs = run_classification(fullscene_df, iterations=20)\n",
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = labels_f1_fs.describe()\n",
    "fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene classification - 80%/20% split - multi-label - CHANCE level\n",
    "\n",
    "The chance level is computed by associating random labels to the testing samples (still following the same distribution of labels as found in the original dataset).\n",
    "\n",
    "Need to implement new chance calculation following recommendations of Reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_chance, labels_f1_fschance = run_classification(fullscene_df, random_labels=True, iterations=20)\n",
    "results_chance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fschance = labels_f1_fschance.describe()\n",
    "fschance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene training; movement alone testing - multi-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels_f1_ma = run_classification(fullscene_df, testing=move_df, iterations=20)\n",
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = labels_f1_ma.describe()\n",
    "ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene training; movement alone testing - multi-labels - CHANCE level\n",
    "\n",
    "The chance level is computed by associating random labels to the testing samples (still following the same distribution of labels as found in the original dataset).\n",
    "\n",
    "Need to implement new chance calculation following recommendations of Reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels_f1_machance = run_classification(fullscene_df, testing=move_df, random_labels=True, iterations=20)\n",
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machance = (labels_f1_machance.describe())\n",
    "machance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure of mean f1 for each label in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mean = fs.iloc[[1]].T\n",
    "ma_mean = ma.iloc[[1]].T\n",
    "\n",
    "f1_mean = pd.concat([fs_mean['mean'], ma_mean['mean']], axis=1, keys=['fullscene', 'movement alone'])\n",
    "f1_mean\n",
    "ax = f1_mean.plot.bar(rot=0, figsize=(7,5)) #plot\n",
    "ax.set_ylabel(\"Mean F1 Score\")\n",
    "ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mean_chance = fschance.iloc[[1]].T\n",
    "ma_mean_chance = machance.iloc[[1]].T\n",
    "f1_mean_chance = pd.concat([fs_mean['mean'], fs_mean_chance['mean'], ma_mean['mean'], ma_mean_chance['mean']], axis=1, keys=['Fullscene', 'Fullscene Chance', 'Movement Alone', 'Movement Alone Chance'])\n",
    "\n",
    "print((f1_mean_chance).round(2).transpose().to_latex(bold_rows=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Factor Analysis\n",
    "\n",
    "Exploratory Factor Analysis examining what latent constructs underlie particiants' responses in each condition.\n",
    "\n",
    "The Python factor_analyzer module is a port of EFA from the R' psych package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import factor_analyzer\n",
    "\n",
    "rotation = 'promax'\n",
    "\n",
    "nb_factors=3\n",
    "\n",
    "efa_fullscene = factor_analyzer.FactorAnalyzer()\n",
    "efa_fullscene.analyze(fullscene_ratings_df, nb_factors, rotation=rotation)\n",
    "fullscene_loadings=efa_fullscene.loadings\n",
    "\n",
    "efa_move = factor_analyzer.FactorAnalyzer()\n",
    "efa_move.analyze(move_ratings_df, nb_factors, rotation=rotation)\n",
    "move_loadings=efa_move.loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the loadings for the *fullscene* vs the *movement alone* data show that the first three factors are highly correlated. **This shows that, using factor analysis, we have uncovered latent constructs that are used by participants to describe the clips in both *fullscene* and *movement alone* conditions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge loadings into one dataframe, movement alone and fullscene side-by-side\n",
    "loadings=pd.concat([fullscene_loadings, move_loadings], keys=[\"fullscene\",\"movement alone\"], axis=1)\n",
    "loadings=loadings.swaplevel(0,1,1).sort_index(axis=1)\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"Pearson correlation between factors 'fullscene' vs 'movement alone'\")\n",
    "for i in range(1, nb_factors+1):\n",
    "    r, p=pearsonr(loadings[\"Factor%d\" % i][\"fullscene\"].values, loadings[\"Factor%d\" % i][\"movement alone\"].values)\n",
    "    print(\"Factor %d: r=%f, p=%f\" % (i,r,p)) \n",
    "    \n",
    "\n",
    "show_heatmap(loadings[abs(loadings)>=0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((loadings[abs(loadings)>=0.3]).round(2).to_latex(bold_rows=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_move.get_factor_variance() #variance explained by each construct for movement-alone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_fullscene.get_factor_variance() #variance explained by each construct for fullscene data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFA embeddings\n",
    "\n",
    "We can use the EFA space as a 'better' space to represent our clips, where the latent, composite constructs correspond to the main axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_factors=6\n",
    "fullscene_efa = np.dot(fullscene,fullscene_loadings.values[:,:nb_of_factors])\n",
    "fullscene_means_efa = np.dot(fullscene_means,fullscene_loadings.values[:,:nb_of_factors])\n",
    "move_efa = np.dot(move,fullscene_loadings.values[:,:nb_of_factors])\n",
    "move_means_efa = np.dot(move_means,fullscene_loadings.values[:,:nb_of_factors])\n",
    "\n",
    "move_pure_efa = np.dot(move,move_loadings.values[:,:nb_of_factors])\n",
    "move_pure_means_efa = np.dot(move_means,move_loadings.values[:,:nb_of_factors])\n",
    "\n",
    "\n",
    "plot_embedding(fullscene_efa, fullscene_labels,fullscene_means_efa, fullscene_means.index, title=\"EFA-space embedding of the fullscene data\", three_d=True)\n",
    "plot_embedding(move_efa, move_labels,move_means_efa, move_means.index, title=\"EFA-space embedding of the movement alone data (EFA on fullscene data)\", three_d=False)\n",
    "plot_embedding(move_pure_efa, move_labels,move_pure_means_efa, move_means.index, title=\"EFA-space embedding of the movement alone data (EFA on movement alone data)\", three_d=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, even if the EFA factors are quite similar, the distances between the same clips in fullscene vs movement alone data are high in the EFA space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_efa=pd.DataFrame(np.power(np.sum(np.power(move_means_efa - fullscene_means_efa, 2), axis=1), 0.5), index=move_means.index, columns=[\"distance_efa\"])\n",
    "\n",
    "print(\"Mean distance:\\n%s\" % distances_efa.mean(axis=0))\n",
    "show_heatmap(distances_efa, cmap=\"summer_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Expressivness of the EFA Embeddings\n",
    "\n",
    "We can now attempt to cluster our 20 clips into 'groups' of similar clips (based on the latent constructs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# kMeans clustering after projecting our clips into the EFA-space\n",
    "fullscene_clustering_data=fullscene_means_efa\n",
    "\n",
    "nb_clusters=7 #arbitrary number of clusters\n",
    "\n",
    "fullscene_kmeans_model = KMeans(n_clusters=nb_clusters, random_state=0).fit(fullscene_clustering_data)\n",
    "fullscene_kmeans = fullscene_kmeans_model.predict(fullscene_clustering_data)\n",
    "\n",
    "plot_embedding(fullscene_clustering_data,fullscene_means.index,clusters=fullscene_kmeans, three_d=True)\n",
    "\n",
    "pd.DataFrame(fullscene_kmeans, index=fullscene_means.index, columns=[\"cluster #\"]).sort_values(by=\"cluster #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to infer the semantics of the first 3 EFA factors.\n",
    "\n",
    "We can then try to predict in which cluster the clips would end up, using only the ratings from the movement alone videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_means_efa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_kmeans= fullscene_kmeans_model.predict(move_means_efa)\n",
    "\n",
    "plot_embedding(fullscene_means_efa, move_means.index, clusters=move_kmeans, three_d=False)\n",
    "\n",
    "diff=pd.DataFrame(fullscene_kmeans-move_kmeans,index=move_means.index)\n",
    "print(\"%d movement alone clips out of %d (%.1f%%) are predicted to fall into the same cluster as their 'fullscene' counterpart.\" % (diff[diff==0].count(), move_kmeans.size, diff[diff==0].count() * 100. / move_kmeans.size))\n",
    "\n",
    "clusters_kripp=pd.DataFrame([fullscene_kmeans, move_kmeans, fullscene_kmeans==move_kmeans,krippendorff_df[[\"Fullscene alpha\", \"Movement-Alone alpha\"]].std(axis=1).astype(float), krippendorff_df[[\"Fullscene alpha\", \"Movement-Alone alpha\"]].mean(axis=1).astype(float), krippendorff_df[\"Fullscene alpha\"], krippendorff_df[\"Movement-Alone alpha\"], ],index=[\"fullscene clusters\", \"movement clusters\", \"same\", \"kripp alpha std\", \"kripp alpha mean\", \"Fullscene alpha\", \"Movement-Alone alpha\"],columns=move_means.index).T.sort_values(by=\"kripp alpha mean\")\n",
    "clusters_kripp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a correlation between 'same clusters' and Krippendorf agreement (ie, consistency of ratings for a given clip)? No..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Krippendorf, same cluster: %f\" % clusters_kripp[clusters_kripp[\"same\"] == True][\"kripp alpha mean\"].mean())\n",
    "print(\"Std Krippendorf, same cluster: %f\" % clusters_kripp[clusters_kripp[\"same\"] == True][\"kripp alpha mean\"].std())\n",
    "print(\"Mean Krippendorf, diff cluster: %f\" % clusters_kripp[clusters_kripp[\"same\"] == False][\"kripp alpha mean\"].mean())\n",
    "print(\"Std Krippendorf, diff cluster: %f\" % clusters_kripp[clusters_kripp[\"same\"] == False][\"kripp alpha mean\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the EFA projections to the original dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullscene_df[\"efa1\"] = pd.Series(fullscene_efa[:,0], index=fullscene_df.index)\n",
    "fullscene_df[\"efa2\"] = pd.Series(fullscene_efa[:,1], index=fullscene_df.index)\n",
    "fullscene_df[\"efa3\"] = pd.Series(fullscene_efa[:,2], index=fullscene_df.index)\n",
    "move_df[\"efa1\"] = pd.Series(move_efa[:,0], index=move_df.index)\n",
    "move_df[\"efa2\"] = pd.Series(move_efa[:,1], index=move_df.index)\n",
    "move_df[\"efa3\"] = pd.Series(move_efa[:,2], index=move_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 20\n",
    "\n",
    "print(\"Fullscene, 80%/20%...\")\n",
    "results_fullscene,labels_f1_fullscene = run_classification(fullscene_df, iterations=nb_iterations)\n",
    "print(\"Fullscene, 80%/20%, EFA space...\")\n",
    "results_fullscene_efa,labels_f1_fullscene_efa = run_classification(fullscene_df, cols=[\"efa1\", \"efa2\", \"efa3\"], iterations=nb_iterations)\n",
    "print(\"Fullscene, chance level...\")\n",
    "results_fullscene_chance,labels_f1_fullscene_chance = run_classification(fullscene_df, random_labels=True, iterations=nb_iterations)\n",
    "print(\"Fullscene, 80%/20%, sanity check [input cols=['age']]...\")\n",
    "results_fullscene_age,labels_f1_fullscene_age = run_classification(fullscene_df, cols=[\"age\"], iterations=nb_iterations)\n",
    "\n",
    "print(\"Fullscene vs skeletons...\")\n",
    "results_fullscene_move,labels_f1_move = run_classification(fullscene_df, testing=move_df, iterations=nb_iterations)\n",
    "print(\"Fullscene vs skeletons, EFA space...\")\n",
    "results_fullscene_move_efa,labels_f1_move_efa = run_classification(fullscene_df, testing=move_df, cols=[\"efa1\", \"efa2\", \"efa3\"], iterations=nb_iterations)\n",
    "print(\"Fullscene vs skeletons, chance level...\")\n",
    "results_fullscene_move_chance,labels_f1_move_chance = run_classification(fullscene_df, testing=move_df, random_labels=True, iterations=nb_iterations)\n",
    "\n",
    "collated_results = pd.DataFrame({\"Full-scene, EFA space\": results_fullscene_efa.mean(),\n",
    "                                 \"Full-scene\": results_fullscene.mean(),\n",
    "                                 \"Full-scene, chance\": results_fullscene_chance.mean(),\n",
    "                                 #\"Full-scene-80-20-sanity-check\": results_fullscene_age.mean(),\n",
    "                                 \"Movement-alone, EFA space\": results_fullscene_move_efa.mean(),\n",
    "                                 \"Movement-alone\": results_fullscene_move.mean(),\n",
    "                                 \"Movement-alone, chance\": results_fullscene_move_chance.mean()})\n",
    "labels_f1 = pd.concat({\"Full-scene, EFA space\": labels_f1_fullscene_efa,\n",
    "                       \"Full-scene\": labels_f1_fullscene,\n",
    "                       \"Full-scene, chance\": labels_f1_fullscene_chance,\n",
    "                       #\"fullscene-80-20-sanity-check\": labels_f1_fullscene_age,\n",
    "                       \"Movement-alone, EFA\": labels_f1_move_efa,\n",
    "                       \"Movement-alone\": labels_f1_move,\n",
    "                       \"Movement-alone, chance\": labels_f1_move_chance}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((collated_results*100).round(1).transpose().to_latex(bold_rows=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullscene = labels_f1.loc[:, 'Full-scene']\n",
    "fullsceneEFA = labels_f1.loc[:, 'Full-scene, EFA space']\n",
    "fullsceneChance = labels_f1.loc[:, 'Full-scene, chance']\n",
    "movement = labels_f1.loc[:, 'Movement-alone']\n",
    "movementEFA = labels_f1.loc[:, 'Movement-alone, EFA']\n",
    "movementChance = labels_f1.loc[:, 'Movement-alone, chance']\n",
    "\n",
    "collated_labels = pd.DataFrame([fullsceneEFA.mean(axis=0)])\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([fullscene.mean(axis=0)])], ignore_index=True)\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([fullsceneChance.mean(axis=0)])], ignore_index=True)\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([movementEFA.mean(axis=0)])], ignore_index=True)\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([movement.mean(axis=0)])], ignore_index=True)\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([movementChance.mean(axis=0)])], ignore_index=True)\n",
    "\n",
    "collated_labels.rename(index={0:'Fullscene, EFA',1:'Fullscene',2:'Chance',\n",
    "                              3:'Movement alone, EFA',4:'Movement alone',5:'Chance'}, inplace=True)\n",
    "collated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((collated_labels*100).round(1).to_latex(bold_rows=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (watfenv)",
   "language": "python",
   "name": "watfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
