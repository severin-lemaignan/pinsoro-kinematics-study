{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Cues on Internal States from the Movements of Natural Social Interactions\n",
    "\n",
    "*This notebook complements and supports the paper \"What Can You See? Identifying Cues on Internal States from the Movements of Natural Social Interactions\" by Bartlett et al.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "#%matplotlib notebook\n",
    "#%matplotlib qt5\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0) # bigger figures\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set() # better looking figs\n",
    "\n",
    "# hide warnings for clarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our own set of small helper functions for plotting, etc\n",
    "from utils import plot_embedding, plot_compare_embeddings, show_heatmap, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset, rename and re-order columns where necessary. *Note that the participants who failed the attention check were excluded, and are not present in the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../fulldata.csv\")\n",
    "\n",
    "# re-order columns + keep only useful ones\n",
    "data = data[['pptID', 'condition', 'age', 'gender', 'nationality', 'firstLang', 'trial', 'clipId', 'freetext',\n",
    " 'q01', 'q02', 'q03', 'q04', 'q05', 'q06', 'q07', 'q08', 'q09', 'q10', 'q11', 'q12', 'q13', 'q14', 'q15', 'q16', 'q17',\n",
    " 'q18', 'q19', 'q20', 'q21', 'q22', 'q23', 'q24', 'q25', 'q26', 'q27', 'q28', 'q29', 'q30']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename `qXX` columns with the names of the actual constructs tested in the questionnaire.\n",
    "\n",
    "Notes:\n",
    "- `condition=1` is the 'Movement-only' (ie, skeletons) condition, `condition=2` is the 'Full-scene' condition\n",
    "- each participant `pptID` watched 4 different clips, hence 4 rows per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "constructs=[\"Sad\", \"Happy\", \"Angry\", \"Excited\", \"Calm\", \"Friendly\", \"Aggressive\", \"Engaged\", \"Distracted\", \"Bored\", \"Frustrated\",\"Dominant\",\"Submissive\"]\n",
    "\n",
    "index = data.columns.tolist()\n",
    "index = index[0:9] + [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + [c for c1 in constructs for c in ['left' + c1, 'right' + c1]]\n",
    "data.columns=index\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "For each left/right pair of constructs, compute the absolute difference and the sum (shifted to [-2, +2] interval).\n",
    "\n",
    "This provides insight on the imbalance of the given construct between the children (difference), and the overall 'strenght' of the construct in the clip (sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in constructs:\n",
    "    data[\"diff\"+c] = abs(data[\"left\" + c] - data[\"right\" + c])\n",
    "    data[\"sum\"+c] = data[\"left\" + c] + data[\"right\" + c] - 4\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 lists of columns names, one for diff/sum constructs (the main one), one for left/right constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnsLeftRight=[]\n",
    "columnsDiffSum=[]\n",
    "\n",
    "for c in constructs:\n",
    "    columnsLeftRight.append(\"left\" + c)\n",
    "    columnsLeftRight.append(\"right\" + c)\n",
    "    \n",
    "    columnsDiffSum.append(\"diff\" + c)\n",
    "    columnsDiffSum.append(\"sum\" + c)\n",
    "\n",
    "\n",
    "# by default, work with differences & sum for each constructs\n",
    "selectedColumns=columnsDiffSum\n",
    "\n",
    "# work with differences & sum and the four questions about group dynamics\n",
    "allQuestionsDiffSum = [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + columnsDiffSum\n",
    "\n",
    "# work with left/right ratings and the four questions about group dynamics i.e. raw ratings\n",
    "allQuestionsLeftRight = [\"Competing\", \"Cooperating\", \"PlaySeparate\", \"PlayTogether\"] + columnsLeftRight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define several useful 'partial' views of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL-SCENE DATA\n",
    "\n",
    "fullscene_df=data[data[\"condition\"]==2] # dataframe showing full scene data only\n",
    "\n",
    "# the responses to the 26 left/right Likert-scale questions\n",
    "fullscene_ratings_df=fullscene_df[selectedColumns].astype(float)\n",
    "fullscene=fullscene_ratings_df.values # the underlying numpy array, needed for clustering\n",
    "\n",
    "# clip names\n",
    "fullscene_labels=fullscene_df[\"clipId\"].values\n",
    "\n",
    "# mean ratings per clip\n",
    "fullscene_means=fullscene_df.groupby([\"clipId\"])[selectedColumns].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOVEMENT-ALONE DATA\n",
    "\n",
    "move_df=data[data[\"condition\"]==1] # dataframe showing movement alone data only\n",
    "\n",
    "# the responses to the 26 left/right Likert-scale questions\n",
    "move_ratings_df=move_df[selectedColumns].astype(float)\n",
    "move=move_ratings_df.values # the underlying numpy array, needed for clustering\n",
    "\n",
    "# clip names\n",
    "move_labels=move_df[\"clipId\"].values\n",
    "\n",
    "# mean ratings per clip\n",
    "move_means=move_df.groupby([\"clipId\"])[selectedColumns].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.1: Inter-Rater Agreement\n",
    "\n",
    "Calculate Kirppendorff's alpha to look at how much participants in each condition agreed on their ratings for each clip (lighter color means higher agreement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import krippendorff\n",
    "\n",
    "krip={}\n",
    "\n",
    "for clipName, group in fullscene_df[[\"clipId\"] + allQuestionsLeftRight].groupby([\"clipId\"]): # working with all raw ratings\n",
    "    krip[int(clipName[0])]=(krippendorff.alpha(group.values[:,1:].astype(int),level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "for clipName, group in move_df[[\"clipId\"] + allQuestionsLeftRight].groupby(\"clipId\"):\n",
    "    krip[clipName]=krip[clipName] + (krippendorff.alpha(group.values[:,1:].astype(int),level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "for clipName, group in move_df[[\"clipId\"] + allQuestionsLeftRight].groupby(\"clipId\"):\n",
    "    ratings = group.values[:,1:].astype(int)\n",
    "    ratings = np.random.randint(0,5,ratings.shape)\n",
    "    krip[clipName]=krip[clipName] + (krippendorff.alpha(ratings,level_of_measurement='interval'), group.shape[0])\n",
    "\n",
    "    \n",
    "krippendorff_df=pd.DataFrame.from_dict(krip,orient=\"index\", columns=[\"Fullscene alpha\", \"N\", \"Movement-Alone alpha\", \"N\",\"Random ratings alpha\", \"N\"])\n",
    "\n",
    "show_heatmap(krippendorff_df[[\"Fullscene alpha\", \"Movement-Alone alpha\", \"Random ratings alpha\"]].round(3), cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the agreement scores in the fullscene vs movement-alone videos, using an paired samples T-test. \n",
    "\n",
    "Question: did participants in the fullscene condition agree more in their ratings of each clip than participants in the movement-alone condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from math import sqrt\n",
    "\n",
    "fullscene_krip = krippendorff_df[\"Fullscene alpha\"]\n",
    "move_krip = krippendorff_df[\"Movement-Alone alpha\"]\n",
    "\n",
    "print('Mean Kripp Alpha Fullscene:', fullscene_krip.mean())\n",
    "print('Mean Kripp Alpha Movement:', move_krip.mean())\n",
    "\n",
    "print('Paired Samples T-Test:', ttest_rel(fullscene_krip, move_krip))\n",
    "\n",
    "cohens_d = (fullscene_krip.mean() - move_krip.mean()) / (sqrt((fullscene_krip.std() ** 2 + move_krip.std() ** 2) / 2))\n",
    "\n",
    "print(\"Cohen's d:\", cohens_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-test comparing krippendorf's alpha in the movement alone condition against 0 to see if there is agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "print('One Sample T-Test:', ttest_1samp(move_krip, 0))\n",
    "\n",
    "cohens_d = (move_krip.mean()-0) / move_krip.std()\n",
    "\n",
    "print(\"Cohen's d:\", cohens_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.2: Automatic Labelling of Social Situations\n",
    "\n",
    "Multi-label classification using k-Nearest Neighbours (k=3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "training_ground_truth = { '1': ['Aggressive'],\n",
    "                         '2': ['Excited', 'Aggressive', 'Aimless'],\n",
    "                         '3': ['Excited', 'Fun'],\n",
    "                         '4': ['Cooperative'],\n",
    "                         '5': ['Bored', 'Aimless'],\n",
    "                         '6': ['Cooperative'],\n",
    "                         '7': ['Dominant'],\n",
    "                         '8': ['Bored', 'Fun'],\n",
    "                         '9': ['Cooperative'],\n",
    "                         '10': ['Cooperative', 'Dominant'],\n",
    "                         '11': ['Cooperative', 'Dominant'],\n",
    "                         '12': ['Aggressive', 'Aimless'],\n",
    "                         '13': ['Excited', 'Aggressive', 'Aimless'],\n",
    "                         '14': ['Aggressive'],\n",
    "                         '15': ['Dominant'],\n",
    "                         '16': ['Cooperative', 'Dominant'],\n",
    "                         '17': ['Excited', 'Aggressive'],\n",
    "                         '18': ['Aggressive', 'Dominant'],\n",
    "                         '19': ['Dominant'],\n",
    "                         '20': ['Excited']}\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(training_ground_truth.values())\n",
    "\n",
    "def create_datasets(training=data, testing=None, cols=allQuestionsDiffSum, test_size=0.2, use_clip_id_as_label=False, random_labels=False, random_state=None):\n",
    "    \"\"\"Returns a training dataset and training labels, and a testing dataset and testing labels.\n",
    "    \n",
    "    If testing is None, it randomly splits the training dataframe (at test_size).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if testing is None:\n",
    "        \n",
    "        if use_clip_id_as_label:\n",
    "            labels = list(training[\"clipId\"].map(int))\n",
    "        else:\n",
    "            labels = []\n",
    "            for id in training[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[str(id)])\n",
    "\n",
    "        data = training[cols].values\n",
    "\n",
    "        training_data, testing_data, training_labels, testing_labels = train_test_split(data, labels, test_size=test_size, random_state=random_state)\n",
    "\n",
    "        if not use_clip_id_as_label:\n",
    "            \n",
    "            training_labels, testing_labels = mlb.transform(training_labels), mlb.transform(testing_labels)\n",
    "            \n",
    "            if random_labels:\n",
    "                for labels in training_labels:\n",
    "                    np.random.shuffle(labels)                 \n",
    "                np.random.shuffle(training_labels)             \n",
    "            \n",
    "\n",
    "        return training_data, testing_data, training_labels, testing_labels\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if use_clip_id_as_label:\n",
    "            training_labels = list(training[\"clipId\"].map(int))\n",
    "            testing_labels = list(testing[\"clipId\"].map(int))\n",
    "        else:\n",
    "            labels = []\n",
    "            for id in training[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[str(id)])\n",
    "\n",
    "            training_labels = mlb.transform(labels)\n",
    "\n",
    "            labels = []\n",
    "            for id in testing[\"clipId\"]:\n",
    "                labels.append(training_ground_truth[str(id)])\n",
    "\n",
    "            testing_labels = mlb.transform(labels)\n",
    "\n",
    "            if random_labels:\n",
    "                if random_labels:\n",
    "                    for labels in training_labels:\n",
    "                        np.random.shuffle(labels)                 \n",
    "                    np.random.shuffle(training_labels) \n",
    "\n",
    "        \n",
    "        training_data = training[cols].values\n",
    "        testing_data = testing[cols].values\n",
    "\n",
    "        return training_data, testing_data, training_labels, testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def train(training_data, training_labels):\n",
    "    \n",
    "    #clf = RandomForestClassifier()\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    #clf = ExtraTreeClassifier(random_state=0)\n",
    "\n",
    "\n",
    "    clf.fit(training_data, training_labels)\n",
    "    \n",
    "    return clf\n",
    "\n",
    "def predict(clf, testing_data, inverse_transform_labels=True):\n",
    "    p = clf.predict(testing_data)\n",
    "    if inverse_transform_labels:\n",
    "        return mlb.inverse_transform(p) \n",
    "    else:\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics as metrics\n",
    "    \n",
    "def run_classification(training, \n",
    "                       testing=None, \n",
    "                       cols=allQuestionsDiffSum, \n",
    "                       use_clip_id_as_label=False, \n",
    "                       random_labels=False,\n",
    "                       crossvalidation_iterations=50):\n",
    "    \"\"\"\n",
    "    Metrics for multi-label classification coming form Sorower, Mohammad S. \"A literature survey on algorithms for multi-label learning.\" Oregon State University, Corvallis (2010).\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\"Accuracy\": [],\n",
    "               \"Precision\": [],\n",
    "               \"Recall\": [],\n",
    "               \"F1-measure\": []}              \n",
    "    labels_f1 = []\n",
    "    \n",
    "\n",
    "    for x in range(crossvalidation_iterations):\n",
    "               \n",
    "        training_data, testing_data, training_labels, testing_labels = create_datasets(training=training, \n",
    "                                                                                       testing=testing, \n",
    "                                                                                       cols=cols, \n",
    "                                                                                       use_clip_id_as_label=use_clip_id_as_label,\n",
    "                                                                                       random_labels=random_labels,\n",
    "                                                                                       random_state = x)\n",
    "        \n",
    "        if x == 0:\n",
    "            print(\"Shape of training data: %s\" % str(training_data.shape))\n",
    "            print(\"Shape of testing data: %s\" % str(testing_data.shape))\n",
    "        \n",
    "        clf = train(training_data, training_labels)\n",
    "\n",
    "        pred_labels = predict(clf, testing_data, inverse_transform_labels = not use_clip_id_as_label)\n",
    "\n",
    "        \n",
    "        at_least_one = 0\n",
    "        at_least_one_no_incorrect = 0\n",
    "        \n",
    "        if not use_clip_id_as_label:\n",
    "            \n",
    "            nb_classes = len(mlb.classes_)\n",
    "            \n",
    "            \n",
    "            labels_f1.append(dict(zip(mlb.classes_, metrics.f1_score(testing_labels, mlb.transform(pred_labels), average=None))))\n",
    "            \n",
    "            results[\"Accuracy\"].append(metrics.accuracy_score(testing_labels, mlb.transform(pred_labels)))\n",
    "            results[\"Recall\"].append(metrics.recall_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            results[\"Precision\"].append(metrics.precision_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            results[\"F1-measure\"].append(metrics.f1_score(testing_labels, mlb.transform(pred_labels), average='weighted'))    \n",
    "            \n",
    "            \n",
    "            \n",
    "            testing_labels = mlb.inverse_transform(testing_labels)\n",
    "            \n",
    "            exact = 0\n",
    "            accuracy = 0\n",
    "            precision = 0\n",
    "            recall = 0\n",
    "            f1_measure = 0\n",
    "            \n",
    "            for actual, pred in zip(testing_labels, pred_labels):\n",
    "                \n",
    "                pred = set(pred)\n",
    "                actual = set(actual)\n",
    "                \n",
    "                if len(pred) == 0: continue\n",
    "                    \n",
    "                if pred == actual:\n",
    "                    #print(\"%s <-> %s\" % (actual, pred))\n",
    "                    exact += 1\n",
    "                    \n",
    "                intersection = pred.intersection(actual)\n",
    "                union = pred.union(actual)\n",
    "\n",
    "                #accuracy += float(len(intersection)) / len(union)\n",
    "                #precision += float(len(intersection)) / len(pred)\n",
    "                #recall += float(len(intersection)) / len(actual)\n",
    "                #f1_measure += 2 * float(len(intersection)) / (len(pred) + len(actual))\n",
    "                \n",
    "            \n",
    "            #results[\"exact\"].append(float(exact) / len(testing_labels))\n",
    "            #results[\"accuracy\"].append(accuracy / len(testing_labels))\n",
    "            #results[\"precision\"].append(precision / len(testing_labels))\n",
    "            #results[\"recall\"].append(recall / len(testing_labels))\n",
    "            #results[\"f1_measure\"].append(f1_measure / len(testing_labels))\n",
    "            \n",
    "            \n",
    "            \n",
    "        else: # use_clip_id_as_label = True\n",
    "            # does not make much sense as at_least_one & at_least_one_no_incorrect are the same as 'exact'\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(results), pd.DataFrame(labels_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of the significance of the classification results, by computing a permutation-based p-value.\n",
    "\n",
    "*This method is based on Ojala and Garriga 2010 \"Permutation Tests for Studying Classifier Performance\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p_value_permutation(dataset, testing=None, k=10, crossvalidation_iterations=50):\n",
    "    \n",
    "    pvalues = []\n",
    "    \n",
    "    for x in range(crossvalidation_iterations):\n",
    "        times_baseline_worst = 0\n",
    "\n",
    "        training_data, testing_data, training_labels, testing_labels = create_datasets(training=dataset,\n",
    "                                                                                       testing=testing,\n",
    "                                                                                       random_state=x)\n",
    "\n",
    "        # train the classifier\n",
    "        clf = train(training_data, training_labels)\n",
    "\n",
    "        # baseline prediction\n",
    "        pred_labels = predict(clf, testing_data, inverse_transform_labels=False)\n",
    "\n",
    "        baseline_error = 1 - metrics.f1_score(testing_labels, pred_labels, average='weighted')\n",
    "\n",
    "        for i in range(k):\n",
    "            testing_labels = np.random.permutation(testing_labels)\n",
    "\n",
    "            error = 1 - metrics.f1_score(testing_labels, pred_labels, average='weighted')\n",
    "\n",
    "            if error <= baseline_error:\n",
    "                times_baseline_worst += 1\n",
    "\n",
    "        pvalues.append((times_baseline_worst + 1)/float(k + 1))\n",
    "    return pd.Series(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene classification - 80%/20% split - multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results, labels_f1_fs = run_classification(fullscene_df, crossvalidation_iterations=300)\n",
    "results.describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the permutation-based p-value of the classifier's chance (300-fold cross-validation). Mean p-value (for 100 permutations) over the 300-fold cross validation: 0.022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_p_value_permutation(fullscene_df, k=100, crossvalidation_iterations=300).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = labels_f1_fs.describe()\n",
    "fs.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene classification - 80%/20% split - multi-label - CHANCE level\n",
    "\n",
    "The chance level is computed by associating random labels to the testing samples (still following the same distribution of labels as found in the original dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_chance, labels_f1_fschance = run_classification(fullscene_df, random_labels=True, crossvalidation_iterations=300)\n",
    "results_chance.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fschance = labels_f1_fschance.describe()\n",
    "fschance.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene training; movement alone testing - multi-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels_f1_ma = run_classification(fullscene_df, testing=move_df, crossvalidation_iterations=300)\n",
    "results.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_p_value_permutation(fullscene_df, testing=move_df, k=100, crossvalidation_iterations=300).describe().round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = labels_f1_ma.describe()\n",
    "ma.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fullscene training; movement alone testing - multi-labels - CHANCE level\n",
    "\n",
    "The chance level is computed by associating random labels to the testing samples (still following the same distribution of labels as found in the original dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, labels_f1_machance = run_classification(fullscene_df, testing=move_df, random_labels=True, crossvalidation_iterations=300)\n",
    "results.describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machance = (labels_f1_machance.describe())\n",
    "machance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure of mean F1-score for each label in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mean = fs.iloc[[1]].T\n",
    "ma_mean = ma.iloc[[1]].T\n",
    "\n",
    "f1_mean = pd.concat([fs_mean['mean'], ma_mean['mean']], axis=1, keys=['fullscene', 'movement alone'])\n",
    "f1_mean\n",
    "ax = f1_mean.plot.bar(rot=0, figsize=(7,5)) #plot\n",
    "ax.set_ylabel(\"Mean F1 Score\")\n",
    "ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_mean_chance = fschance.iloc[[1]].T\n",
    "ma_mean_chance = machance.iloc[[1]].T\n",
    "f1_mean_chance = pd.concat([fs_mean['mean'], fs_mean_chance['mean'], ma_mean['mean'], ma_mean_chance['mean']], axis=1, keys=['Fullscene', 'Fullscene Chance', 'Movement Alone', 'Movement Alone Chance'])\n",
    "\n",
    "(f1_mean_chance).round(3).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Factor Analysis\n",
    "\n",
    "Exploratory Factor Analysis examining what latent constructs underlie particiants' responses in each condition.\n",
    "\n",
    "The Python factor_analyzer module is a port of EFA from the R' psych package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import factor_analyzer\n",
    "\n",
    "rotation = 'promax'\n",
    "\n",
    "nb_factors=3\n",
    "\n",
    "efa_fullscene = factor_analyzer.FactorAnalyzer()\n",
    "efa_fullscene.analyze(fullscene_ratings_df, nb_factors, rotation=rotation)\n",
    "fullscene_loadings=efa_fullscene.loadings\n",
    "\n",
    "efa_move = factor_analyzer.FactorAnalyzer()\n",
    "efa_move.analyze(move_ratings_df, nb_factors, rotation=rotation)\n",
    "move_loadings=efa_move.loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the loadings for the *fullscene* vs the *movement alone* data show that the first three factors are highly correlated. **This shows that, using factor analysis, we have uncovered latent constructs that are used by participants to describe the clips in both *fullscene* and *movement alone* conditions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge loadings into one dataframe, movement alone and fullscene side-by-side\n",
    "loadings=pd.concat([fullscene_loadings, move_loadings], keys=[\"fullscene\",\"movement alone\"], axis=1)\n",
    "loadings=loadings.swaplevel(0,1,1).sort_index(axis=1)\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"Pearson correlation between factors 'fullscene' vs 'movement alone'\")\n",
    "for i in range(1, nb_factors+1):\n",
    "    r, p=pearsonr(loadings[\"Factor%d\" % i][\"fullscene\"].values, loadings[\"Factor%d\" % i][\"movement alone\"].values)\n",
    "    print(\"Factor %d: r=%f, p=%f\" % (i,r,p)) \n",
    "    \n",
    "\n",
    "show_heatmap(loadings[abs(loadings)>=0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_move.get_factor_variance() #variance explained by each construct for movement-alone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_fullscene.get_factor_variance() #variance explained by each construct for fullscene data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFA embeddings\n",
    "\n",
    "We can use the EFA space as a 'better' space to represent our clips, where the latent, composite constructs correspond to the main axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_of_factors=6\n",
    "fullscene_efa = np.dot(fullscene,fullscene_loadings.values[:,:nb_of_factors])\n",
    "fullscene_means_efa = np.dot(fullscene_means,fullscene_loadings.values[:,:nb_of_factors])\n",
    "move_efa = np.dot(move,fullscene_loadings.values[:,:nb_of_factors])\n",
    "move_means_efa = np.dot(move_means,fullscene_loadings.values[:,:nb_of_factors])\n",
    "\n",
    "move_pure_efa = np.dot(move,move_loadings.values[:,:nb_of_factors])\n",
    "move_pure_means_efa = np.dot(move_means,move_loadings.values[:,:nb_of_factors])\n",
    "\n",
    "\n",
    "plot_embedding(fullscene_efa, fullscene_labels,fullscene_means_efa, fullscene_means.index, title=\"EFA-space embedding of the fullscene data\", three_d=True)\n",
    "plot_embedding(move_efa, move_labels,move_means_efa, move_means.index, title=\"EFA-space embedding of the movement alone data (EFA on fullscene data)\", three_d=False)\n",
    "plot_embedding(move_pure_efa, move_labels,move_pure_means_efa, move_means.index, title=\"EFA-space embedding of the movement alone data (EFA on movement alone data)\", three_d=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, even if the EFA factors are quite similar, the distances between the same clips in fullscene vs movement alone data are high in the EFA space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_efa=pd.DataFrame(np.power(np.sum(np.power(move_means_efa - fullscene_means_efa, 2), axis=1), 0.5), index=move_means.index, columns=[\"distance_efa\"])\n",
    "\n",
    "print(\"Mean distance:\\n%s\" % distances_efa.mean(axis=0))\n",
    "show_heatmap(distances_efa, cmap=\"summer_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Expressivness of the EFA Embeddings\n",
    "\n",
    "We can now attempt to cluster our 20 clips into 'groups' of similar clips (based on the latent constructs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# kMeans clustering after projecting our clips into the EFA-space\n",
    "fullscene_clustering_data=fullscene_means_efa\n",
    "\n",
    "nb_clusters=7 #arbitrary number of clusters\n",
    "\n",
    "fullscene_kmeans_model = KMeans(n_clusters=nb_clusters, random_state=0).fit(fullscene_clustering_data)\n",
    "fullscene_kmeans = fullscene_kmeans_model.predict(fullscene_clustering_data)\n",
    "\n",
    "plot_embedding(fullscene_clustering_data,fullscene_means.index,clusters=fullscene_kmeans, three_d=True)\n",
    "\n",
    "pd.DataFrame(fullscene_kmeans, index=fullscene_means.index, columns=[\"cluster #\"]).sort_values(by=\"cluster #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to infer the semantics of the first 3 EFA factors.\n",
    "\n",
    "We can then try to predict in which cluster the clips would end up, using only the ratings from the movement alone videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_means_efa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_kmeans= fullscene_kmeans_model.predict(move_means_efa)\n",
    "\n",
    "plot_embedding(fullscene_means_efa, move_means.index, clusters=move_kmeans, three_d=False)\n",
    "\n",
    "diff=pd.DataFrame(fullscene_kmeans-move_kmeans,index=move_means.index)\n",
    "print(\"%d movement alone clips out of %d (%.1f%%) are predicted to fall into the same cluster as their 'fullscene' counterpart.\" % (diff[diff==0].count(), move_kmeans.size, diff[diff==0].count() * 100. / move_kmeans.size))\n",
    "\n",
    "clusters_kripp=pd.DataFrame([fullscene_kmeans, move_kmeans, fullscene_kmeans==move_kmeans,krippendorff_df[[\"Fullscene alpha\", \"Movement-Alone alpha\"]].std(axis=1).astype(float), krippendorff_df[[\"Fullscene alpha\", \"Movement-Alone alpha\"]].mean(axis=1).astype(float), krippendorff_df[\"Fullscene alpha\"], krippendorff_df[\"Movement-Alone alpha\"], ],index=[\"fullscene clusters\", \"movement clusters\", \"same\", \"kripp alpha std\", \"kripp alpha mean\", \"Fullscene alpha\", \"Movement-Alone alpha\"],columns=move_means.index).T.sort_values(by=\"kripp alpha mean\")\n",
    "clusters_kripp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a correlation between 'same clusters' and Krippendorf agreement (ie, consistency of ratings for a given clip)? No..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Krippendorf, same cluster: %f\" % clusters_kripp[clusters_kripp[\"same\"] == True][\"kripp alpha mean\"].mean())\n",
    "print(\"Std Krippendorf, same cluster: %f\" % clusters_kripp[clusters_kripp[\"same\"] == True][\"kripp alpha mean\"].std())\n",
    "print(\"Mean Krippendorf, diff cluster: %f\" % clusters_kripp[clusters_kripp[\"same\"] == False][\"kripp alpha mean\"].mean())\n",
    "print(\"Std Krippendorf, diff cluster: %f\" % clusters_kripp[clusters_kripp[\"same\"] == False][\"kripp alpha mean\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the EFA projections to the original dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullscene_df[\"efa1\"] = pd.Series(fullscene_efa[:,0], index=fullscene_df.index)\n",
    "fullscene_df[\"efa2\"] = pd.Series(fullscene_efa[:,1], index=fullscene_df.index)\n",
    "fullscene_df[\"efa3\"] = pd.Series(fullscene_efa[:,2], index=fullscene_df.index)\n",
    "move_df[\"efa1\"] = pd.Series(move_efa[:,0], index=move_df.index)\n",
    "move_df[\"efa2\"] = pd.Series(move_efa[:,1], index=move_df.index)\n",
    "move_df[\"efa3\"] = pd.Series(move_efa[:,2], index=move_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then re-classify the clips, comparing the performance of the original 26-dimensional ratings to the 3-dimensional EFA-space projections (*still using a 300-fold cross-validation)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 300\n",
    "\n",
    "print(\"Fullscene, 80%/20%...\")\n",
    "results_fullscene,labels_f1_fullscene = run_classification(fullscene_df, crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene, 80%/20%, EFA space...\")\n",
    "results_fullscene_efa,labels_f1_fullscene_efa = run_classification(fullscene_df, cols=[\"efa1\", \"efa2\", \"efa3\"], crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene, chance level...\")\n",
    "results_fullscene_chance,labels_f1_fullscene_chance = run_classification(fullscene_df, random_labels=True, crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene, 80%/20%, sanity check [input cols=['age']]...\")\n",
    "results_fullscene_age,labels_f1_fullscene_age = run_classification(fullscene_df, cols=[\"age\"], crossvalidation_iterations=nb_iterations)\n",
    "\n",
    "print(\"Fullscene vs skeletons...\")\n",
    "results_fullscene_move,labels_f1_move = run_classification(fullscene_df, testing=move_df, crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene vs skeletons, EFA space...\")\n",
    "results_fullscene_move_efa,labels_f1_move_efa = run_classification(fullscene_df, testing=move_df, cols=[\"efa1\", \"efa2\", \"efa3\"], crossvalidation_iterations=nb_iterations)\n",
    "print(\"Fullscene vs skeletons, chance level...\")\n",
    "results_fullscene_move_chance,labels_f1_move_chance = run_classification(fullscene_df, testing=move_df, random_labels=True, crossvalidation_iterations=nb_iterations)\n",
    "\n",
    "collated_results = pd.DataFrame({\"Full-scene, EFA space\": results_fullscene_efa.mean(),\n",
    "                                 \"Full-scene\": results_fullscene.mean(),\n",
    "                                 \"Full-scene, chance\": results_fullscene_chance.mean(),\n",
    "                                 #\"Full-scene-80-20-sanity-check\": results_fullscene_age.mean(),\n",
    "                                 \"Movement-alone, EFA space\": results_fullscene_move_efa.mean(),\n",
    "                                 \"Movement-alone\": results_fullscene_move.mean(),\n",
    "                                 \"Movement-alone, chance\": results_fullscene_move_chance.mean()})\n",
    "labels_f1 = pd.concat({\"Full-scene, EFA space\": labels_f1_fullscene_efa,\n",
    "                       \"Full-scene\": labels_f1_fullscene,\n",
    "                       \"Full-scene, chance\": labels_f1_fullscene_chance,\n",
    "                       #\"fullscene-80-20-sanity-check\": labels_f1_fullscene_age,\n",
    "                       \"Movement-alone, EFA\": labels_f1_move_efa,\n",
    "                       \"Movement-alone\": labels_f1_move,\n",
    "                       \"Movement-alone, chance\": labels_f1_move_chance}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collated_results.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullscene = labels_f1.loc[:, 'Full-scene']\n",
    "fullsceneEFA = labels_f1.loc[:, 'Full-scene, EFA space']\n",
    "fullsceneChance = labels_f1.loc[:, 'Full-scene, chance']\n",
    "movement = labels_f1.loc[:, 'Movement-alone']\n",
    "movementEFA = labels_f1.loc[:, 'Movement-alone, EFA']\n",
    "movementChance = labels_f1.loc[:, 'Movement-alone, chance']\n",
    "\n",
    "collated_labels = pd.DataFrame([fullsceneEFA.mean(axis=0)])\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([fullscene.mean(axis=0)])], ignore_index=True)\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([fullsceneChance.mean(axis=0)])], ignore_index=True)\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([movementEFA.mean(axis=0)])], ignore_index=True)\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([movement.mean(axis=0)])], ignore_index=True)\n",
    "collated_labels = pd.concat([collated_labels, pd.DataFrame([movementChance.mean(axis=0)])], ignore_index=True)\n",
    "\n",
    "collated_labels.rename(index={0:'Fullscene, EFA',1:'Fullscene',2:'Chance',\n",
    "                              3:'Movement alone, EFA',4:'Movement alone',5:'Chance'}, inplace=True)\n",
    "collated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (watfenv)",
   "language": "python",
   "name": "watfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
